{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Decision Tree Implementation From Scratch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libaries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "\t# Formula - -pi * log(pi)\n",
    "\t# pi = number1/(number1 + number2)\n",
    "\tdef calculate_entropy(self,num,denominator):\n",
    "\t     \n",
    "\t     pi = num/denominator\n",
    "\t     if pi == 0:\n",
    "\t     \t# To avoid divided by zero when calculating np.log2\n",
    "\t     \treturn 0\n",
    "\t     else:\t\n",
    "\t     \treturn -pi*np.log2(pi)\n",
    "\n",
    "\t# Calculate E(Target) - Entropy of Target\n",
    "\tdef calculate_target_entropy(self,vector):\n",
    "\t\tentropy = 0\n",
    "\t\tvalues = vector.value_counts()\n",
    "\t\ttotal = len(vector)\n",
    "\t\tfor value in values:\n",
    "\t\t\tentropy += self.calculate_entropy(value,total)\n",
    "\t\treturn entropy\t\n",
    "\n",
    "\t# Calculate E(Target | Attribute) - Entropy of Features\n",
    "\tdef calculate_attribute_entropy(self,dataset,attribute,target):\n",
    "\t\ttargets = dataset[target].unique()\n",
    "\t\tattribute_vector = dataset[attribute]\n",
    "\t\t\n",
    "\t\ttotal_samples = len(attribute_vector)\n",
    "\t\tproperties = attribute_vector.unique()\n",
    "\t\tentropy = 0\n",
    "\t\tfor prop in properties:\n",
    "\t\t\tprop_entropy = 0\n",
    "\t\t\tdenominator = len(dataset[attribute][ dataset[attribute] == prop ])\n",
    "\t\t\tfor target_class in targets:\n",
    "\t\t\t\tnumber = len(dataset[attribute][ dataset[attribute] == prop ][dataset[target] == target_class ])\n",
    "\t\t\t\tprop_entropy += self.calculate_entropy(number,denominator)\n",
    "\n",
    "\t\t\tp_attribute = denominator/total_samples\n",
    "\t\t\tentropy += p_attribute*prop_entropy\n",
    "\t\treturn entropy\n",
    "\n",
    "\t# Calculate Information Gain of Features\t\n",
    "\t# Formula = E(Target) - E(Target| Attribute)\n",
    "\tdef calculate_information_gain(self,dataset,attribute,target):\n",
    "\t\ttarget_entropy = self.calculate_target_entropy(dataset[target])\n",
    "\t\tattribute_entropy = self.calculate_attribute_entropy(dataset,attribute,target)\n",
    "\t\treturn target_entropy - attribute_entropy\n",
    "\n",
    "\n",
    "\t# Find out decision node by calulating max Information Gain\n",
    "\tdef winner_attribute(self,df):\n",
    "\t\t\n",
    "\t\tinformation_gain = []\n",
    "\t\ttarget = df.keys()[-1]\n",
    "\t\tfeatures =  df.keys()[:-1] # Exclude the last one.\n",
    "\t\t\n",
    "\t\tfor feature in features: \n",
    "\t\t\tinformation_gain.append(self.calculate_information_gain(df,feature,target))\n",
    "\t\t\n",
    "\t\tmaximum_ig_index = np.argmax(information_gain)\n",
    "\t\twinner_feature = features[maximum_ig_index]\n",
    "\t\treturn winner_feature\n",
    "\n",
    "\t# Split the dataset on decision node\t\n",
    "\tdef split_dataset(self,df,node,value):\n",
    "\t\treturn df[df[node] == value].reset_index(drop=True)\n",
    "\n",
    "\t# Build Decision Tree\n",
    "\tdef build_tree(self,df,tree=None):\n",
    "\t\ttarget_class = df.keys()[-1]\n",
    "\t\tnode = self.winner_attribute(df)\n",
    "\t\tnode_values= df[node].unique()\n",
    "\t\tif tree is None:\n",
    "\t\t\ttree= {}\n",
    "\t\t\ttree[node] = {}\n",
    "\t\tfor value in node_values:\n",
    "\t\t\tsubtable = self.split_dataset(df,node,value)\n",
    "\t\t\tsubset_target_class = subtable[target_class].unique()\n",
    "\t\t\tif len(subset_target_class) == 1:\n",
    "\t\t\t\ttree[node][value] = subset_target_class[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttree[node][value] = self.build_tree(subtable)\n",
    "\n",
    "\t\treturn tree\n",
    "\n",
    "\t# Start training process. Ultimate goal is to make a decision tree.\n",
    "\tdef fit(self,df):\n",
    "\t\tself.tree = self.build_tree(df)\t\n",
    "\n",
    "\n",
    "\t# Traverse through decision tree.\n",
    "\tdef traverse_tree(self,guess,tree):\n",
    "\n",
    "\t   prediction = ''\n",
    "\t   for node in tree.keys():\n",
    "\t   \tvalue = guess[node]\n",
    "\t   \ttree = tree[node][value]\n",
    "\t   \n",
    "\t   \tif type(tree) is dict:\n",
    "\t   \t\tprediction = self.traverse_tree(guess,tree)\n",
    "\t   \telse:\n",
    "\t   \t\tprediction = tree\n",
    "\t   \t\tbreak\n",
    "\n",
    "\t   return prediction\n",
    "\n",
    "\n",
    "\t# Predict the class using Input values\n",
    "\tdef predict(self,guess):\n",
    "\n",
    "\t   prediction = ''\n",
    "\t   tree = self.tree\n",
    "\t   prediction = self.traverse_tree(guess,tree)\n",
    "\t   return prediction\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data - (weight (kg), Height (cm), gender)\n",
    "dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into Dataframe\n",
    "dataframe = pd.DataFrame(dataset,columns=['Taste','Temperature','Texture','Eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem - Should I eat if taste is salty, temperature is hot and texture is hard?\n",
    "data = {'Taste':'Salty','Temperature':'Hot','Texture':'Hard'}\n",
    "guess = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SVM Model\n",
    "model = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "model.fit(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "prediction = model.predict(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
